import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os
import datetime
import sys

class LLMEngine:
    def __init__(self, model_path="models/sera_adapter"):
        print("\nâš™ï¸  Sera AI Motoru (CPU) YÃ¼kleniyor... LÃ¼tfen bekleyin.")
        
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        self.model_path = os.path.join(base_dir, "models", "sera_adapter")
        
        self.base_model_name = "unsloth/Qwen2.5-3B-Instruct"
        self.device = "cpu"
        
        try:
            self.base_model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                torch_dtype=torch.float32,
                device_map=self.device,
                trust_remote_code=True
            )
        except Exception as e:
            print(f"âŒ Ana Model YÃ¼kleme HatasÄ±: {e}")
            raise e

        if os.path.exists(self.model_path):
            print(f"ğŸ”— Sera KiÅŸiliÄŸi BaÄŸlanÄ±yor...")
            try:
                self.model = PeftModel.from_pretrained(self.base_model, self.model_path)
                self.model = self.model.merge_and_unload()
                print("âœ… AdaptÃ¶r baÅŸarÄ±yla birleÅŸtirildi.")
            except Exception as e:
                print(f"âš ï¸ AdaptÃ¶r yÃ¼klenirken hata: {e}\nVarsayÄ±lan model kullanÄ±lÄ±yor.")
                self.model = self.base_model
        else:
            print(f"âš ï¸  UYARI: AdaptÃ¶r bulunamadÄ± ({self.model_path})! VarsayÄ±lan model Ã§alÄ±ÅŸacak.")
            self.model = self.base_model

        self.model.eval()
        self.tokenizer = AutoTokenizer.from_pretrained(self.base_model_name)
        print("âœ… Sera HazÄ±r ve Emrinizde!\n")

    def generate_response(self, user_input, context=""):
        now = datetime.datetime.now()
        tarih_saat = now.strftime("%d %B %Y, Saat %H:%M")

        system_prompt = (
            f"Åu anki tarih: {tarih_saat}.\n"
            "Senin adÄ±n Sera. Utku Kalender tarafÄ±ndan geliÅŸtirilen, yerel aÄŸda Ã§alÄ±ÅŸan asistanÄ±msÄ±n.\n"
            "Sorulara kÄ±sa, net ve yardÄ±msever TÃ¼rkÃ§e cevaplar ver.\n"
            "ASLA hashtag (#), etiket listesi veya gereksiz emoji yÄ±ÄŸÄ±nÄ± kullanma."
        )

        if context:
            system_prompt += f"\n\nEK BÄ°LGÄ° (HafÄ±za):\n{context}"

        full_prompt = f"""AÅŸaÄŸÄ±da bir gÃ¶revi tanÄ±mlayan bir talimat ve baÄŸlam saÄŸlayan bir girdi bulunmaktadÄ±r. Ä°steÄŸi uygun ÅŸekilde tamamlayan bir yanÄ±t yazÄ±n.

### Instruction:
{system_prompt}
KullanÄ±cÄ± Soru: {user_input}

### Input:

### Response:
"""
        try:
            inputs = self.tokenizer(full_prompt, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=250,
                    temperature=0.6,
                    do_sample=True,
                    repetition_penalty=1.2
                )
                
            full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            if "### Response:" in full_text:
                response = full_text.split("### Response:")[-1].strip()
            else:
                response = full_text

            
            if "#" in response:
                response = response.split("#")[0].strip()

            response = response.replace("Intel", "Utku Kalender")
            response = response.replace("OpenAI", "Utku Kalender")
            response = response.replace("tarafÄ±ndan geliÅŸtirilen bir yapay zeka modeliyim", "Utku Kalender tarafÄ±ndan geliÅŸtirilen Sera'yÄ±m")

            return response
            
        except Exception as e:
            print(f"âŒ Hata: {e}")
            return "Beynimde anlÄ±k bir iÅŸlem hatasÄ± oluÅŸtu Utku."

if __name__ == "__main__":
    motor = LLMEngine()
    print("Sera:", motor.generate_response("Merhaba, seni kim yaptÄ±?"))
